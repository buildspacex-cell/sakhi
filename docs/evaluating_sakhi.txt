Evaluating Sakhi’s Conversational Architecture
Current Architecture Overview
Listening Layer (Input & NLU):
 
Sakhi’s pipeline begins with a
 
listening layer
, which captures the user’s input (voice or text) and converts it into a machine-readable form. This layer includes Automatic Speech Recognition (ASR) if voice is used, followed by Natural Language Understanding (NLU) to interpret the input
[1]
[2]
. The NLU component extracts the user’s
 
intent(s)
 
and any relevant
 
slots/entities
 
from the utterance. Sakhi’s design appears to distinguish between
 
outer intents
 
(the high-level goal or mode of the conversation, e.g. entering a journaling session vs. scheduling a task) and
 
inner intents
 
(more granular actions or sub-intents within that context). In practice, this means the system first classifies what broad context the user is in (or attempting to enter), then identifies any specific request within that context. For example, the user might be in a “journaling” outer intent, but the inner intent could be expressing a particular feeling or asking a question. The listening layer likely uses a combination of intent classification and slot-filling: once an intent is recognized, any required details (slots) are gathered from user input in a frame-based manner. This is typical of form-based dialogue design, where the bot prompts for missing information to complete an action
[3]
. While this structured approach ensures needed data is collected, it can also make interactions feel a bit rigid if overused
[3]
.
Emotional Context Processing:
 
Alongside intent recognition, Sakhi’s NLU incorporates
 
emotion detection
 
to gauge the user’s sentiment or emotional state. This could involve a pre-trained emotion classification model (for instance, a BERT-based classifier fine-tuned on an emotion dataset) that labels the user’s utterance with emotions like joy, sadness, frustration, etc
[4]
[5]
. Sakhi uses this emotional context to modulate its responses – a strategy often called
 
emotion-aware tone generation
. In essence, the conversational layer knows
 
how
 
to say something, not just
 
what
 
to say, by adjusting wording, formality, and empathy level based on the user’s mood. For example, if the user sounds upset or says something indicating sadness, Sakhi might respond with a softer, more empathetic tone (short sentences, gentle acknowledgments, words of encouragement). Conversely, if the user is cheerful, Sakhi’s tone can be upbeat and playful. One implementation approach is to include the detected emotion labels as part of the input to the language generation model, or to select from pre-written response templates tagged for certain emotions
[5]
. The goal is to make the interaction feel supportive and human-like, aligning with Sakhi’s role as an emotionally intelligent companion.
Conversing Layer (Dialogue Management):
 
At the core is Sakhi’s
 
dialogue manager
, which maintains context and controls the flow of conversation
[6]
. This layer handles
 
dialog frames
 
– essentially the conversational context or “topic” the user and assistant are currently engaged in. Sakhi’s design likely uses a frame or state representing the current activity (for instance, a Journaling frame vs. a Planning frame). Within a frame, the system expects certain sub-intents and slots. The
 
outer intent
 
determines the frame and overall script (e.g. the journaling schema of questions/answers), while
 
inner intents
 
cover the user’s moves within that frame (e.g. answering a journaling prompt, asking for advice, or changing topic). The dialogue manager uses the history of the interaction (dialogue state) to decide how to respond next – whether to continue with a prompt, provide a reflection, switch context, or trigger an action. Ideally, it should also manage
 
context carryover
, meaning if the user provided some information earlier, the system remembers it and doesn’t ask again unnecessarily
[7]
[8]
.
In Sakhi’s current logic, conversation flow might be managed through a combination of state machines and hierarchical if/else logic. For example, if the outer intent is “journaling” and the user has not yet provided today’s feeling, the system knows to ask that (slot filling). If the user diverts (say, asks an unrelated question), Sakhi’s manager attempts an
 
interruption handling
: possibly treating the new query as a temporary inner intent switch. However, handling such context switches is non-trivial. Humans naturally pause one topic to address another and then return to the original topic
[9]
. Sakhi’s architecture tries to mimic this by
 
pausing the current frame
, processing the interrupting intent, and then resuming the prior frame. This requires storing the state of the first conversation, so it can be restored after the interruption
[10]
. For instance, if Sakhi was in the middle of a guided journaling session and the user suddenly asks “What’s the weather tomorrow?”, Sakhi should momentarily set aside the journaling context, address the weather question (perhaps via a “faq/weather” intent), then bring the conversation back to where it left off (“As you were saying about your day…”). The existing system attempts this by saving slot values and state before switching intents, then later checking if there’s a saved state to restore
[10]
[8]
.
Journaling Layer:
 
A key feature of Sakhi is helping the user reflect through guided journaling. The
 
journaling layer
 
can be seen as a specialized part of the conversing layer, with its own schema or script. An internal design thread (“Journaling schema layer design”) suggests that Sakhi’s journaling is implemented via a structured schema – possibly a predefined sequence of prompts or a template for the journaling conversation. In the current architecture, this schema might be tightly integrated with the dialogue logic (for example, a hardcoded sequence of frames/slots: ask about today’s highlight, ask about challenge, etc.). The journaling layer uses the emotional context and user’s responses to generate reflective feedback. It listens (like an active listener) when the user is venting or explaining, then might summarize or rephrase the user’s feelings to show understanding (
“It sounds like you felt unappreciated at work today.”
). It also likely logs these exchanges in a
 
journal entry
 
data store – creating a record that the user (and Sakhi) can review later. However, because of how it’s implemented, the
 
journaling schema is closely coupled
 
with the state machine: the system might be coded to follow a particular journaling flow, which can make it hard to modify that flow without changing code. This tight coupling could mean limited flexibility – for example, skipping a question or handling an out-of-order user narrative might not fit the rigid schema.
“Hands” Layer (Action Planning & Execution):
 
Beyond listening and conversing, Sakhi aims to
 
help users plan and take action
 
(the assistant’s metaphorical “hands”). This involves scheduling events, setting reminders, or breaking down goals into tasks. The architecture includes a component that, upon recognizing an intent to plan or schedule, interfaces with external tools or services. For instance, if during conversation the user says,
 
“I want to start exercising three times a week,”
 
Sakhi’s inner intent might classify this as a planning request. The hands layer would then engage – perhaps by asking follow-up questions (slots like preferred time, specific days) and then creating a schedule or to-do entry. In the current setup, this might be done via direct integration with a calendar or task management API. The
 
action planning
 
logic likely constructs a plan (e.g. a workout schedule) and confirms with the user before saving it. However, the
 
integration
 
may not be fully seamless yet – it might be more of a prototype or rule-based action (for example, just summarizing a plan in text rather than actually booking calendar slots). The concept is similar to a skill-based architecture where certain intents route to utility functions. A robust approach is to have a clear boundary here: the dialogue manager decides
 
when
 
to invoke the hands layer, and the hands layer executes the requested action (like scheduling) through an API or database call
[11]
. In Sakhi’s vision, this layer ensures the companion not only empathizes and chats, but also
 
helps the user achieve goals
 
(turning talk into tangible outcomes).
Robustness and Alignment with Sakhi’s Vision
Sakhi’s overarching vision is to be a conversational companion that
 
“listens, reflects, and helps users plan.”
 
This requires a delicate balance between
 
human-like responsiveness
 
and
 
predictable, controlled behavior
. The current architecture embodies this vision to some extent: it has components for empathetic listening (emotion-aware conversing) and practical planning (hands/action layer). The question is whether the current design is
 
robust and scalable
 
enough, and how well it aligns with future goals like deeper personalization and long-term user growth.
On the positive side, the layered approach (listening → conversing → journaling/planning) ensures some separation of concerns. Using intents and slots provides
 
predictability
 
– the system knows what information it needs and what action to perform. The emotion-aware response generation addresses the need for human-like empathy, making interactions warmer and more engaging than a sterile assistant. Additionally, having a journaling schema shows a commitment to
 
longitudinal interaction
: Sakhi isn’t just handling isolated queries, but guiding recurring sessions that build on past conversations (e.g. yesterday’s journal vs today’s). This is a step toward the long-term relationship and insight-building that Sakhi aims for.
However, there are clear
 
limitations in the current design
 
that hinder robustness and scalability:
Dialog Frame Handling & Context Carryover:
 
The current frame-based flow may struggle with fluid conversation. If the user deviates from the expected script (e.g., an interruption or a non-sequitur comment), the system has to juggle multiple contexts. There is evidence that without careful state management, the assistant might drop the original context or force the user to repeat information. In human dialogue, participants seamlessly switch context and then return to the prior topic
[9]
. Sakhi’s present implementation relies on manual preservation of state (saving slot data in session attributes) to mimic this
[10]
. This approach can be brittle – for instance, if a developer forgets to save a certain variable, the context could be lost. Additionally, the frame “stack” might only be one level deep; multiple nested interruptions could confuse it. A robust system needs to support stacking and restoring arbitrarily nested contexts (within reason) without confusion.
Multiple Intent Management:
 
Users often convey multiple intentions in one utterance or rapidly successive ones (e.g.,
 
“I had a bad day at work, and I also need to plan my grocery shopping.”
). The current intent classification might only pick one “outer intent” at a time, potentially ignoring secondary intents. There is no sophisticated multi-intent parsing in place — which means Sakhi could miss opportunities to address all facets of the user’s input, or it might erroneously handle a compound request as if it were a single topic. Best practices in dialog systems suggest using advanced NLU and context tracking to
 
differentiate and handle multiple intents
 
gracefully
[12]
. Sakhi’s design so far is likely single-intent-focused per turn, which limits its conversational agility. Moreover, quickly switching intents (from journaling to scheduling and back) might not be smooth; the user might feel the system “forgets” what they said if context isn’t preserved across the switch.
State Transitions and Dialogue Flow:
 
The architecture currently may rely on a finite-state or scripted flow for guided dialogs (especially journaling). Finite-state dialog management works for constrained scenarios, but it can become unwieldy as the conversation paths grow complex
[13]
. If Sakhi’s developers keep adding new intents and special cases, the state graph could explode in complexity, making it hard to maintain or predict. There might also be issues with how transitions happen: for example, after finishing a journaling session, does Sakhi smoothly transition to open conversation or suggest an action? If the user abruptly ends a session, can the system gracefully close it out? In the current design, these transitions might be handled in an ad-hoc manner (with flags or checks like “if journaling active vs not”). A more scalable design would have a clear unified state representation and transition logic that can accommodate new states without breaking existing ones.
Journaling Schema Coupling:
 
As hinted earlier, the journaling flow in the current system is likely hardwired to a particular schema (set sequence of prompts and expected intents). This tight coupling means
 
limited flexibility
: if we want to change the journaling questions, incorporate a new type of journal entry, or personalize the flow for different users, it would require code changes and might introduce bugs. It also may mix the content with logic – the system might directly reference specific prompt IDs or sequence numbers in the code. This is not ideal for maintainability. Additionally, because journaling is somewhat siloed as an “outer intent,” there may be duplication of functionality that exists elsewhere (for example, the logic for empathizing with a user’s problem might be duplicated in both normal chat and journaling modes). Such coupling can slow down future development – any evolution in the journaling feature might ripple through the conversation manager unexpectedly.
Handling of Interruptions and Topic Shifts:
 
While the design tries to handle interruptions, it might not fully cover the breadth of human conversation shifts. Users might change topic without explicit markers, or they might partially shift (halfway through answering a journal question they digress to another story). The current architecture might either (a) rigidly steer them back, which could feel unnatural, or (b) follow the tangent and lose track of the original thread. Neither is ideal. The challenge is to allow Sakhi to be
 
adaptive
: follow the user on a tangent if it seems important (especially in emotional conversations) but also gently remind or return to the main thread later. This level of nuance is hard to achieve with simple state machines; it leans into needing more dynamic dialogue policies or AI-driven management.
Memory and Personalization Limits:
 
Sakhi’s current memory of the user may be mostly contained in the journaling logs or short-term context from recent turns. There isn’t an advanced long-term memory module indicated in the current design (besides perhaps storing journal entries). As a result, Sakhi might not yet identify patterns over time or update its behavior to suit the individual user deeply. Truly personalized companions require distinguishing and retaining
 
meaningful insights
 
from the conversation history (for example, remembering a user’s core values, recurring challenges, or life events)
[14]
. The architecture should eventually support transforming raw conversation data into
 
persistent knowledge
 
about the user. Right now, if that knowledge exists, it’s likely not structured or being actively used in dialog decisions. This limits Sakhi’s ability to “build longitudinal insight,” like noticing
 
“You seem happier lately than you were last month”
 
or recalling
 
“Last week you planned to practice guitar – how did that go?”
 
Such features align with Sakhi’s vision but demand a more sophisticated memory and profiling system than the current setup.
In summary, the existing architecture provides a foundation and partial implementation of Sakhi’s goals, but it shows
 
fragility and inflexibility
 
in managing complex, real-world conversations. It leans a bit too much on hard-coded patterns (intents, frames) which can conflict with the fluidity of actual human dialogue. This can undermine robustness (system might get confused by unexpected input) and make scaling up to new features or more users difficult (because adding states/intents increases complexity non-linearly
[13]
). Next, we outline recommendations for a clearer, maintainable end-to-end design that would overcome these limitations.
Recommendations for a More Scalable Design
To better balance natural interaction with controlled behavior, Sakhi’s architecture should evolve towards a more modular, context-aware, and flexible
 
dialogue system design
. Below are key recommendations:
Adopt a Hierarchical Dialogue Manager:
 
Introduce a clearer separation between high-level dialogue flows and sub-dialogues. Instead of a monolithic state machine, use a
 
hierarchical state management
 
or a
 
stack-based context manager
. The system can treat distinct activities (free chat, journaling, planning, Q&A, etc.) as sub-dialog “frames” that can be pushed and popped as needed. For example, when a journaling session is initiated, push a Journaling dialogue module onto the stack. If an interruption (like a quick question) comes, push that as a new context, handle it, then pop it to return to journaling. This design, analogous to how Alexa Skills handle context switching, ensures the assistant does not lose earlier slots or progress
[7]
[8]
. It also simplifies tracking: only the top of the stack is active, but others are paused in memory. Many modern frameworks encourage such patterns for managing nested intents and context switching smoothly. By structuring the dialog manager hierarchically, Sakhi can more easily support
 
interruptions and topic shifts
 
without hard-coded logic for each case.
Frame and State Representation:
 
Define a unified data structure for the conversation state (e.g., a JSON or dict that holds current intent, slots filled, active frame, any paused frames, and context variables like detected emotion). This state should be passed and updated with each turn, rather than using many global flags. Having an explicit
 
dialogue state object
 
makes it easier to preserve context across turns and even sessions (if saved). It also makes the conversation logic more transparent and debuggable. Developers can inspect this state to understand what the bot
 
thinks
 
is going on. Moreover, with a well-defined state, implementing transitions becomes a matter of updating this object and possibly writing high-level transition functions (like
 
enterFrame(X)
,
 
exitFrame()
, etc.) instead of scattering transition rules in many places. In a scalable design, adding a new frame (say, a new kind of guided exercise or a different structured activity) would involve defining its schema and possibly handlers, but the underlying manager would handle entering/exiting it in a uniform way.
Decouple Content Schema from Code:
 
Refactor the journaling (and any similar scripted dialogue) so that its
 
schema is data-driven
. The sequence of journal prompts, for instance, could live in a configuration file or database table that the system reads. The dialog manager can iterate through this schema, but it doesn’t need to hardcode each step. This makes it far easier to modify the flow or have multiple journaling templates (perhaps a different one for end-of-day reflection vs. stress management). The code should handle generic behaviors (like “after user answers, reflect empathetically” or “if user skips, handle accordingly”) without being tied to a specific prompt. A data-driven approach reduces
 
coupling
 
and improves maintainability – one can update the journaling questions or add a new slot without touching the core logic. It also aligns with future personalization: you might customize the journaling schema per user (based on what works for them) simply by loading a different schema, all while using the same engine. In general, strive for a
 
separation of content and logic
, where possible.
Enhance Multi-Intent Handling:
 
Upgrade Sakhi’s NLU to detect and handle multiple intents in a single utterance when appropriate. This could involve multi-label classification or sequential intent parsing. If a user message contains two clear requests or a statement plus a request, the system can either address them one by one or ask the user to clarify which to focus on first. The dialogue manager should be able to queue up intents or outcomes rather than just dropping one. At the very least, implement
 
advanced NLU and context management techniques
 
to ensure no important user goal is ignored
[12]
. For instance, a user saying
 
“I’m feeling down, and I have a big project tomorrow”
 
might require both empathy (journaling/empathetic response) and planning (perhaps help preparing for the project) – Sakhi could first comfort the user, then proactively ask if they want to make a plan for the project. Achieving this fluidity may involve complex intent policies or even leveraging an LLM’s capability to interpret nuance, but it is key for a truly
 
assistant-like
 
experience.
Modular “Skills” or Action Handlers:
 
Structure the action-oriented capabilities (the “hands” layer) as modular
 
skills
 
or functions that the dialogue manager can invoke. For scheduling, for example, have a dedicated scheduler module. For to-do list management, a tasks module, etc. The dialogue manager’s job is to route the intent to the correct module and then supervise the conversation to complete that task (collect parameters, confirm execution). This is akin to how virtual assistants incorporate skills or mini-apps
[15]
[11]
. Such a design will be more scalable because new action capabilities can be added as new modules without altering the core conversation flow. It also encapsulates external API calls or tool usage. In Sakhi’s case, a planning request would trigger the Planning module which handles all scheduling logic (dates, reminders, integration with calendar API) and returns a result or confirmation to the dialog manager. The benefit is twofold:
 
predictable control
 
(each skill is coded for a specific domain, reducing unpredictable AI behavior when executing tasks) and maintainability (the planning logic can be improved independently of the main conversational logic).
Emotion & Tone Handling Improvements:
 
Continue to leverage emotion detection, but consider a more sophisticated
 
emotion-to-language
 
pipeline. For instance, maintain a running estimate of the user’s emotional state that decays over time rather than resetting each turn. This would give Sakhi a bit of “mood memory” – if a user has been sad for the last few turns, even a neutral utterance might merit a gentle tone until a positive change is observed. Also, expand the repertoire of tone-adjusted responses. One approach (already in use in some prototypes) is prompt engineering for the generative model: include the emotion as part of the prompt and use examples to shape the style
[5]
. Another approach is templating: for each dialog act, have variations like
 
[neutral_version, empathetic_version, enthusiastic_version]
 
and choose based on emotion. The key recommendation is to ensure this tone generation remains
 
modular
 
– e.g., a dedicated function or module that takes a base reply and the emotional context, and outputs a final polished reply. This keeps the concern of “how to say it” separate from “what to say”, making the system easier to tweak. It will also aid personalization: some users might respond better to a very upbeat coach-like tone, others to a calm therapeutic tone – you could adjust tone profiles per user in the future, which fits Sakhi’s personalization goal
[16]
.
Integrate Long-Term Memory and Personalization:
 
Begin building a
 
long-term memory store
 
for Sakhi to accumulate knowledge about the user across sessions. This doesn’t have to be one monolithic memory; it can include several facets – e.g., a semantic memory of facts (user’s preferences, key biographical info), a preference model (likes/dislikes, communication style), and a summary memory of past conversations
[14]
[17]
. Modern agent architectures, such as Amazon’s AgentCore Memory, illustrate the importance of extracting and consolidating meaningful information from dialogues into a knowledge base
[14]
[18]
. Sakhi should similarly filter out trivial chat from important revelations. For example, if in a journaling session the user reveals
 
“I often procrastinate because I feel overwhelmed,”
 
that’s a crucial insight to store. The architecture can incorporate a background process that after each conversation (or key turn) updates the user’s profile – perhaps summarizing that day’s journal entry or noting any new goal the user set. Over time, Sakhi will have a rich context to draw on, enabling more
 
personalized and insightful interactions
. Concretely, this could be a database or even a vector store for embeddings of past dialogues, paired with metadata. The dialogue manager can query this memory for relevant info (with user consent/privacy considerations) to inform responses. For instance, before planning a new goal, Sakhi might recall
 
“In the past, you struggled when you took on too much at once”
 
and then advise accordingly. Designing the memory subsystem with efficient retrieval and updating in mind is important – you might employ embeddings for semantic search of relevant past items, and keep the most salient facts readily accessible
[19]
. Ensuring the
 
continuity of context across sessions
 
will make Sakhi truly feel like a long-term companion rather than a reset bot each time
[19]
.
Use a Hybrid Control/NLG Approach:
 
To keep responses both coherent and natural, consider a hybrid architecture combining rule-based planning with learned language generation. For example, the dialogue manager can decide
 
what the bot should do or say
 
at a high level (this guarantees coherence and relevance), then use a neural language model to generate the actual sentence with the desired tone and details. This yields predictability (since the content is grounded in the manager’s decision) while leveraging the richness of an AI model for phrasing. Many assistants use this pattern: the system might internally create a
 
dialogue act
 
like
 
AFFIRMATION(user_goal="exercise", tone="encouraging")
 
and then have the NLG component realize it into a sentence:
 
“I’m really glad you’re focusing on exercise – I believe you can do it!”
 
The developer retains control over the intent and slots of the message, but not every possible sentence needs to be handwritten, allowing variety. This approach also simplifies localization or style changes down the line. Essentially, treat the LLM or template engine as a
 
surface realization
 
tool guided by a controlled intermediate representation. By doing so, Sakhi can upgrade its language quality and human-likeness without sacrificing the frame/slot structure that ensures it stays on track.
Plan for Continuous Learning and Improvement:
 
As a final note, build hooks into the system for monitoring and improving performance. The architecture should log interactions (with privacy safeguards) in a way that developers or automated evaluators can review them. Difficult dialogues, user corrections, or disengagement moments are valuable data. Incorporate a feedback loop where either the system self-evaluates (via heuristics or model-based critique) or developers periodically retrain components (NLU, NLG) on collected data. This ensures the
 
assistant evolves
 
and stays aligned with user needs. For example, if analysis of journals shows that users often stop the session at a certain question, that might indicate a design issue – which can be fixed in the schema without changing core code if we follow the decoupling advice above. Furthermore, as Sakhi starts to handle more diverse conversations, the system could benefit from machine learning policies (e.g., reinforcement learning or transformer-based dialogue models) to decide on responses in open-ended situations. A modular architecture allows plugging such components in gradually: one could experiment with an ML-based
 
dialogue policy
 
for small talk or empathetic listening, while keeping planning strictly rule-based. This kind of layered AI approach (rules for high precision tasks, ML for high recall/naturalness tasks) will support Sakhi’s growth into a more sophisticated companion.
By implementing these recommendations, Sakhi’s system would become
 
more robust, scalable, and aligned with its vision
. The assistant would handle conversation flows more gracefully (no more getting stuck or derailed by an unexpected comment), manage user context over long stretches, and be easier to extend with new capabilities. It would balance
 
human-like warmth
 
with a solid
 
architectural backbone
 
that developers can trust. In practical terms, users would experience Sakhi as a more seamless companion – one that remembers their story, adapts to their needs, allows interruptions just like a human would, and helps them achieve goals in a consistent yet personalized manner. This clearer end-to-end design, emphasizing modularity and memory, sets the stage for Sakhi to eventually provide
 
deeper personalization and longitudinal insights
, fulfilling its role as an enduring supportive presence in the user’s life.
Sources:
Alexa Blogs –
 
“Build for Context Switching: Don’t Forget Important Information”
 
(example of handling interruptions and resuming context)
[9]
[10]
Django Stars –
 
“Dialog Management in Chatbot Development”
 
(overview of dialogue manager approaches and best practices)
[13]
[12]
Hiten Mahajan –
 
“Building an Emotion-Aware Chatbot”
 
(using emotion classification and tone-conditioned generation in chatbots)
[5]
[16]
Amazon AWS –
 
“AgentCore Memory: Long-Term Memory Deep Dive”
 
(insights on persistent memory for AI agents)
[14]
[17]
Macaron AI –
 
“Virtual AI Assistant Playbook”
 
(architecture of virtual assistants, memory, and skill modules)
[19]
[11]
Alexa Blogs –
 
“Remembering and Restoring Context with Session Attributes”
 
(technique for saving slot data to handle context carryover)
[7]
[8]
Django Stars –
 
“Finite-State vs. Form-Based Dialogs”
 
(limitations of rigid state machines and form filling in complex conversations)
[13]
[3]
[1]
 
[2]
 
[3]
 
[6]
 
[12]
 
[13]
 
Dialog Management in Chatbots. Essential Guide
https://djangostars.com/blog/dialog-management-chatbot-development/
[4]
 
[5]
 
[16]
 
Building an Emotion-Aware Chatbot | by Hiten Mahajan | Medium
https://medium.com/@hiten016/building-an-emotion-aware-chatbot-d7e109e4cabd
[7]
 
[8]
 
[9]
 
[10]
 
Build for Context Switching: Don't Forget Important Information When Switching Between Intents : Alexa Blogs
https://developer.amazon.com/en-US/blogs/alexa/post/114cec18-4a38-4cbe-8c6b-0fa6d8413f4f/build-for-context-switching-don-t-forget-important-information-when-switching-between-intent
[11]
 
[15]
 
[19]
 
The Virtual AI Assistant Playbook: Architecture, Use‑Cases, and ROI - Macaron
https://macaron.im/blog/macaron-ai-virtual-assistant-playbook
[14]
 
[17]
 
[18]
 
Building smarter AI agents: AgentCore long-term memory deep dive | Artificial Intelligence
https://aws.amazon.com/blogs/machine-learning/building-smarter-ai-agents-agentcore-long-term-memory-deep-dive/